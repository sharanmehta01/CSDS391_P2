Explaining the neural network architecture (file csds391_p2.ipynb)


Preprocessing the data -  
We use tensorflow (integrated with keras) to set up our neural network. Our goal is to train the model on the full iris data set (3 species, 4 attributes). 

After importing the required packages, we read the CSV file using pandas and rename each of the speicies as floats (setosa = 0.0, versicolor = 1.0, virginica = 2.0) to avoid erros regaridng mixed data types.

We create two numpy arrays, one for the four different attriubtes/features over which the model needs to be trained and one for the labels (name of the speicies itself).

An important part of of preprocessing the data is normalizing the x-axis vaues. This transforms the range from 0.0-8.0 to 0.0-1.0 and makes it easier for the model to learn. The normalization is done using the MinMaxScaler

The other important part regarding setting up data is to shuffle it once. This will help with validation later on. 


Traiining the model - 
We use a sequential model with 2 hidden layers with the third layer being the output layer. The output layer uses the softmax activation to produce probabilites.

We set our model optimizer to be SGD (Stochastic Gradient Descent) rather than the default Adam optimizer. Our lerning rate is set at 0.05 and we use sparse categorical crossentropy for our loss calculation.

Next we set the x values as the iris features and the y values as the iris labels. We have 15 batches with each batch learning 10 samples at once and this is done 50 times. 

We took out 0.1 (10%) of the shuffled data as a sample to validate the model later on. This means that this 10% or 15 samples is previously unseen by the model and we can use it to test how well the model can predict their labels. 

Shuffle is set to true to prevent the model from simply learning the order of the features and labels.

After 50 epochs we can see that our model is trained and is 97.78% accurate. The validation accuracy of 100% shows the the model truly is well trained. 



Contrasting with K-means clustering (file csds391-p2_kmeans) - 
We next set up a k-means clustering model with 3 clusters. We don't train this model but let it start predicting on its own. 

We see the output array after the model is passed over the full iris dataset and the results seem fairly good with most of the samples that were close together idenitifes as being under the same speicies. 

K-means is model that doesn't work of off historical data but rather groups samples with similar attritubes together. 
The neural network model on the other hand learns based on historical data and can therefore recognize that some of the far plotted samples are also part of the same species (since it may have seen those in the training sample too). 

The k-means data has roughly a 78% success rate which is much lower than the 3 layer neural network. This is to be expected though since there are quite a few samples that fall far from their labels and into the descion boundaries of other means. 

A point to recognize though is that the validation sample for the neural network was picked randomly. This means that it could have included all points that are very close to their means and are easier to predict. K-means does not have such a benefit. 
However, is it also possilbe for cases to exist where the training data included no far away points and the validation data are filled with sample points that have a low probabiltiy of being in the same group due the the training data. 



